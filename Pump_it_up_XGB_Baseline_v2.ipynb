{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hellojohnkim/mmai894/blob/main/Pump_it_up_XGB_Baseline_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "65vWvm-ZGHt0"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import hyperopt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
        "# for feature engineering\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from feature_engine import imputation as mdi\n",
        "from feature_engine import discretisation as dsc\n",
        "from feature_engine import encoding as ce\n",
        "from feature_engine.selection import (\n",
        "    DropConstantFeatures,\n",
        "    DropDuplicateFeatures,\n",
        "    SmartCorrelatedSelection,\n",
        ")\n",
        "from sklearn.impute import KNNImputer\n",
        "from feature_engine.imputation import CategoricalImputer\n",
        "from feature_engine.outliers import Winsorizer\n",
        "from feature_engine.encoding import CountFrequencyEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from feature_engine.outliers import Winsorizer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, r2_score, f1_score\n",
        "from itertools import combinations\n",
        "import pickle\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from hyperopt import hp, tpe, Trials\n",
        "import hyperopt\n",
        "from hyperopt.fmin import generate_trials_to_calculate\n",
        "from functools import partial\n",
        "import warnings, os\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ccPpyCghGo2D"
      },
      "outputs": [],
      "source": [
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaloidJpXoIO"
      },
      "source": [
        "##Pump It Up Faulty Water Pump Prediction Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1rDzc_2YTAY"
      },
      "source": [
        "#Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcRkDDUjGcN6",
        "outputId": "4200ce57-6864-434c-9da9-c490045bc45e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 59400 entries, 0 to 59399\n",
            "Data columns (total 41 columns):\n",
            " #   Column                 Non-Null Count  Dtype  \n",
            "---  ------                 --------------  -----  \n",
            " 0   id                     59400 non-null  int64  \n",
            " 1   amount_tsh             59400 non-null  float64\n",
            " 2   date_recorded          59400 non-null  object \n",
            " 3   funder                 55763 non-null  object \n",
            " 4   gps_height             59400 non-null  int64  \n",
            " 5   installer              55745 non-null  object \n",
            " 6   longitude              59400 non-null  float64\n",
            " 7   latitude               59400 non-null  float64\n",
            " 8   wpt_name               59398 non-null  object \n",
            " 9   num_private            59400 non-null  int64  \n",
            " 10  basin                  59400 non-null  object \n",
            " 11  subvillage             59029 non-null  object \n",
            " 12  region                 59400 non-null  object \n",
            " 13  region_code            59400 non-null  int64  \n",
            " 14  district_code          59400 non-null  int64  \n",
            " 15  lga                    59400 non-null  object \n",
            " 16  ward                   59400 non-null  object \n",
            " 17  population             59400 non-null  int64  \n",
            " 18  public_meeting         56066 non-null  object \n",
            " 19  recorded_by            59400 non-null  object \n",
            " 20  scheme_management      55522 non-null  object \n",
            " 21  scheme_name            30590 non-null  object \n",
            " 22  permit                 56344 non-null  object \n",
            " 23  construction_year      59400 non-null  int64  \n",
            " 24  extraction_type        59400 non-null  object \n",
            " 25  extraction_type_group  59400 non-null  object \n",
            " 26  extraction_type_class  59400 non-null  object \n",
            " 27  management             59400 non-null  object \n",
            " 28  management_group       59400 non-null  object \n",
            " 29  payment                59400 non-null  object \n",
            " 30  payment_type           59400 non-null  object \n",
            " 31  water_quality          59400 non-null  object \n",
            " 32  quality_group          59400 non-null  object \n",
            " 33  quantity               59400 non-null  object \n",
            " 34  quantity_group         59400 non-null  object \n",
            " 35  source                 59400 non-null  object \n",
            " 36  source_type            59400 non-null  object \n",
            " 37  source_class           59400 non-null  object \n",
            " 38  waterpoint_type        59400 non-null  object \n",
            " 39  waterpoint_type_group  59400 non-null  object \n",
            " 40  status_group           59400 non-null  object \n",
            "dtypes: float64(3), int64(7), object(31)\n",
            "memory usage: 18.6+ MB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 14850 entries, 0 to 14849\n",
            "Data columns (total 40 columns):\n",
            " #   Column                 Non-Null Count  Dtype  \n",
            "---  ------                 --------------  -----  \n",
            " 0   id                     14850 non-null  int64  \n",
            " 1   amount_tsh             14850 non-null  float64\n",
            " 2   date_recorded          14850 non-null  object \n",
            " 3   funder                 13980 non-null  object \n",
            " 4   gps_height             14850 non-null  int64  \n",
            " 5   installer              13973 non-null  object \n",
            " 6   longitude              14850 non-null  float64\n",
            " 7   latitude               14850 non-null  float64\n",
            " 8   wpt_name               14850 non-null  object \n",
            " 9   num_private            14850 non-null  int64  \n",
            " 10  basin                  14850 non-null  object \n",
            " 11  subvillage             14751 non-null  object \n",
            " 12  region                 14850 non-null  object \n",
            " 13  region_code            14850 non-null  int64  \n",
            " 14  district_code          14850 non-null  int64  \n",
            " 15  lga                    14850 non-null  object \n",
            " 16  ward                   14850 non-null  object \n",
            " 17  population             14850 non-null  int64  \n",
            " 18  public_meeting         14029 non-null  object \n",
            " 19  recorded_by            14850 non-null  object \n",
            " 20  scheme_management      13881 non-null  object \n",
            " 21  scheme_name            7608 non-null   object \n",
            " 22  permit                 14113 non-null  object \n",
            " 23  construction_year      14850 non-null  int64  \n",
            " 24  extraction_type        14850 non-null  object \n",
            " 25  extraction_type_group  14850 non-null  object \n",
            " 26  extraction_type_class  14850 non-null  object \n",
            " 27  management             14850 non-null  object \n",
            " 28  management_group       14850 non-null  object \n",
            " 29  payment                14850 non-null  object \n",
            " 30  payment_type           14850 non-null  object \n",
            " 31  water_quality          14850 non-null  object \n",
            " 32  quality_group          14850 non-null  object \n",
            " 33  quantity               14850 non-null  object \n",
            " 34  quantity_group         14850 non-null  object \n",
            " 35  source                 14850 non-null  object \n",
            " 36  source_type            14850 non-null  object \n",
            " 37  source_class           14850 non-null  object \n",
            " 38  waterpoint_type        14850 non-null  object \n",
            " 39  waterpoint_type_group  14850 non-null  object \n",
            "dtypes: float64(3), int64(7), object(30)\n",
            "memory usage: 4.5+ MB\n"
          ]
        }
      ],
      "source": [
        "#Load files\n",
        "data_path = '/Users/johnkim/Downloads/'\n",
        "#For train set, merge the values and label on id\n",
        "train = pd.merge(pd.read_csv(data_path + 'training_set_values.csv'), pd.read_csv(data_path + 'training_set_label.csv'), on=['id'])\n",
        "test = pd.read_csv(data_path + 'test_set.csv')\n",
        "train.info()\n",
        "test.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oz01plLwJe1o"
      },
      "outputs": [],
      "source": [
        "X = train[train.columns[:-1]]\n",
        "y = train['status_group']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DujZCjTLLWRX",
        "outputId": "4116d783-fbfa-4620-bb64-ff2d467da217"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "RangeIndex: 59400 entries, 0 to 59399\n",
            "Series name: status_group\n",
            "Non-Null Count  Dtype \n",
            "--------------  ----- \n",
            "59400 non-null  object\n",
            "dtypes: object(1)\n",
            "memory usage: 464.2+ KB\n"
          ]
        }
      ],
      "source": [
        "y.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkR9TqckYega"
      },
      "source": [
        "#Data Pre-Processing & Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IhWtLvPTLEo2"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "# Calculate 'age'\n",
        "current_year = datetime.now().year\n",
        "for dataset in [X, test]:\n",
        "    dataset['age'] = current_year - dataset['construction_year']\n",
        "    dataset['age'].replace({current_year: np.nan}, inplace=True)  # Replace invalid ages with NaN\n",
        "\n",
        "X.drop(['construction_year'], axis=1, inplace=True)\n",
        "test.drop(['construction_year'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0BLrq9EOJwrG"
      },
      "outputs": [],
      "source": [
        "class FrequencyEncoder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, single_columns=None, multi_columns=None):\n",
        "        self.single_columns = single_columns if single_columns is not None else []\n",
        "        self.multi_columns = multi_columns if multi_columns is not None else []\n",
        "        self.freq_maps = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Single column frequency encoding\n",
        "        for col in self.single_columns:\n",
        "            self.freq_maps[col] = X[col].value_counts().to_dict()\n",
        "\n",
        "        for cols in self.multi_columns:\n",
        "            grouped = X.groupby(list(cols)).size()\n",
        "            self.freq_maps[tuple(cols)] = {index: count for index, count in grouped.items()}  # changed from iteritems() to items()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_transformed = X.copy()\n",
        "        # Apply single column frequency encoding\n",
        "        for col in self.single_columns:\n",
        "            X_transformed[col + '_Freq'] = X_transformed[col].map(self.freq_maps[col])\n",
        "\n",
        "        # Apply multi-column frequency encoding\n",
        "        for cols in self.multi_columns:\n",
        "            col_name = '_'.join(cols) + '_Freq'\n",
        "            X_transformed[col_name] = X_transformed[list(cols)].apply(\n",
        "                lambda row: self.freq_maps[tuple(cols)].get(tuple(row), 0), axis=1)\n",
        "\n",
        "        return X_transformed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WILGVc-yKge-"
      },
      "outputs": [],
      "source": [
        "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, columns_to_drop):\n",
        "        self.columns_to_drop = columns_to_drop\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_transformed = X.drop(self.columns_to_drop, axis=1)\n",
        "        return X_transformed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qVilrOnAME9i"
      },
      "outputs": [],
      "source": [
        "def preprocess_dataset(X):\n",
        "    # Store the 'ID' column\n",
        "    X_id = X['id']\n",
        "    # Temporarily remove the 'ID' column\n",
        "    X = X.drop(['id'], axis=1)\n",
        "\n",
        "    # One-Hot Encoding for categorical variables\n",
        "    obj_columns = X.select_dtypes(include='object').columns.tolist()\n",
        "    encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
        "    X_encoded = encoder.fit_transform(X[obj_columns])\n",
        "    with open('encoder.pkl', 'wb') as file:\n",
        "        pickle.dump(encoder, file)\n",
        "\n",
        "    # Convert encoded data back to DataFrame\n",
        "    encoded_col_names = encoder.get_feature_names_out(obj_columns)\n",
        "    X_encoded_df = pd.DataFrame(X_encoded, columns=encoded_col_names)\n",
        "\n",
        "    X_preprocessed = pd.concat([X, X_encoded_df], axis=1)\n",
        "\n",
        "    # Add 'ID' back to the front of the DataFrame\n",
        "    X_preprocessed.insert(0, 'id', X_id.reset_index(drop=True))\n",
        "\n",
        "    return X_preprocessed\n",
        "\n",
        "# Apply the preprocessing to the entire dataset\n",
        "X_train_preprocessed = preprocess_dataset(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#X_train_preprocessed.isnull().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 59400 entries, 0 to 59399\n",
            "Columns: 66215 entries, id to waterpoint_type_group_other\n",
            "dtypes: float64(66179), int64(6), object(30)\n",
            "memory usage: 29.3+ GB\n"
          ]
        }
      ],
      "source": [
        "X_train_preprocessed.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "X_train_preprocessed.drop(['id'], axis=1, inplace=True)\n",
        "\n",
        "def objective(space):\n",
        "    # Instantiate the FrequencyEncoder with the required columns\n",
        "    freq_encoder = FrequencyEncoder(\n",
        "    single_columns = ['quantity_group'],\n",
        "    multi_columns = [('waterpoint_type', 'extraction_type_class')])\n",
        "\n",
        "    columns_to_drop = X_train_preprocessed.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "    # Create a pipeline that includes the FrequencyEncoder and XGBoost classifier\n",
        "    pipeline = Pipeline([\n",
        "        ('freq_encoder', freq_encoder),\n",
        "        ('column_dropper', ColumnDropper(columns_to_drop)),\n",
        "        ('xgb_classifier', xgb.XGBClassifier(\n",
        "            n_estimators=int(space['n_estimators']),\n",
        "            colsample_bytree=space['colsample_bytree'],\n",
        "            learning_rate=space['learning_rate'],\n",
        "            max_depth=int(space['max_depth']),\n",
        "            min_child_weight=space['min_child_weight'],\n",
        "            subsample=space['subsample'],\n",
        "            gamma=space['gamma'],\n",
        "            reg_lambda=space['reg_lambda'],\n",
        "            reg_alpha=space['reg_alpha'],\n",
        "            n_jobs=-1,\n",
        "            use_label_encoder=False,  # Added to avoid a warning in newer versions\n",
        "            eval_metric='logloss'     # Added to specify the evaluation metric\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    # Using cross-validation with a classification scoring metric\n",
        "    scores = cross_val_score(pipeline, X_train_preprocessed, y, cv=5, scoring='accuracy')  # Change scoring to 'accuracy' or other suitable metric\n",
        "\n",
        "    # Average score across folds\n",
        "    average_score = scores.mean()\n",
        "    print(\"Average Score:\", average_score)\n",
        "\n",
        "    return {'loss': 1 - average_score, 'status': STATUS_OK}\n",
        "\n",
        "space = {\n",
        "    'learning_rate': hp.loguniform('x_learning_rate', np.log(0.01), np.log(0.12)),\n",
        "    'max_depth': hp.quniform(\"x_max_depth\", 3, 10, 1),\n",
        "    'min_child_weight': hp.quniform('x_min_child', 1, 6, 1),\n",
        "    'subsample': hp.uniform('x_subsample', 0.5, 1),\n",
        "    'gamma': hp.loguniform('x_gamma', np.log(0.01), np.log(5)),\n",
        "    'colsample_bytree': hp.uniform('x_colsample_bytree', 0.5, 1),\n",
        "    'reg_lambda': hp.loguniform('x_reg_lambda', np.log(1), np.log(100)),\n",
        "    'reg_alpha': hp.loguniform('x_reg_alpha', np.log(0.01), np.log(100)),\n",
        "    'n_estimators': hp.choice('n_estimators', np.arange(50, 1000, 50, dtype=int))\n",
        "}\n",
        "\n",
        "best = fmin(fn=objective,\n",
        "            space=space,\n",
        "            algo=tpe.suggest,\n",
        "            max_evals=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fv8P8FkZLfp"
      },
      "source": [
        "#XGB Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypJt0qYOKiJl",
        "outputId": "68b20367-c3bb-4ecd-9b7f-0015856a628d"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Creating a baseline XGBoost model\n",
        "xgb_baseline = XGBClassifier(random_state=42)\n",
        "\n",
        "# Retraining the XGBoost model with the re-processed data\n",
        "xgb_baseline.fit(X_train, y_train_labels)\n",
        "\n",
        "# Making predictions on the validation set\n",
        "y_val_pred = xgb_baseline.predict(X_val)\n",
        "\n",
        "# Recalculating baseline performance\n",
        "accuracy_val = accuracy_score(y_val_labels, y_val_pred)\n",
        "classification_rep_val = classification_report(y_val_labels, y_val_pred, output_dict=True)\n",
        "\n",
        "# Convert the classification report to a DataFrame\n",
        "classification_report_df = pd.DataFrame(classification_rep_val).transpose()\n",
        "\n",
        "# Display the classification report and accuracy\n",
        "print(classification_report_df)\n",
        "print(f\"Validation Accuracy: {accuracy_val:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iis4BsAfZQTb"
      },
      "source": [
        "#XGB Baseline + Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHBn-PwRLift",
        "outputId": "0485c766-dfcb-4f90-9be0-6b44ad99e87c"
      },
      "outputs": [],
      "source": [
        "# Tweaking hyperparameters of the XGBoost model\n",
        "xgb_tweaked = XGBClassifier(\n",
        "    n_estimators=1000,  # Increasing the number of trees\n",
        "    learning_rate=0.1,  # Adjusting learning rate\n",
        "    max_depth=8,  # Setting a maximum depth for trees\n",
        "    subsample=0.8,  # Using 80% of data for fitting individual trees\n",
        "    colsample_bytree=0.8,  # Using 80% of features for constructing each tree\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Retraining the model with tweaked hyperparameters\n",
        "xgb_tweaked.fit(X_train, y_train_labels)\n",
        "\n",
        "# Making predictions on the validation set with the tweaked model\n",
        "y_val_pred_tweaked = xgb_tweaked.predict(X_val)\n",
        "\n",
        "# Recalculating performance with the tweaked model\n",
        "accuracy_val_tweaked = accuracy_score(y_val_labels, y_val_pred_tweaked)\n",
        "classification_rep_val_tweaked = classification_report(y_val_labels, y_val_pred_tweaked, output_dict=True)\n",
        "\n",
        "# Convert the classification report to a DataFrame\n",
        "classification_report_df = pd.DataFrame(classification_rep_val_tweaked).transpose()\n",
        "\n",
        "# Display the classification report and accuracy\n",
        "print(classification_report_df)\n",
        "print(f\"Validation Accuracy: {accuracy_val_tweaked:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpbUdb0cQobY"
      },
      "source": [
        "still need to submit to driven data and get test results\n",
        "\n",
        "### Baseline Model: XGBoost with Tweaked Hyperparameters\n",
        "\n",
        "**Model Configuration:**\n",
        "- **n_estimators**: 1000 (Number of trees increased)\n",
        "- **learning_rate**: 0.1\n",
        "- **max_depth**: 8\n",
        "- **subsample**: 0.8 (80% of data used for fitting individual trees)\n",
        "- **colsample_bytree**: 0.8 (80% of features used for constructing each tree)\n",
        "- **random_state**: 42\n",
        "\n",
        "**Model Performance on Validation Set:**\n",
        "- **Accuracy**: 80.61%\n",
        "\n",
        "**Classification Report:**\n",
        "\n",
        "| Class | Precision | Recall  | F1-Score | Support    |\n",
        "|-------|-----------|---------|----------|------------|\n",
        "| 0     | 0.8033    | 0.8936  | 0.8460   | 6457.000   |\n",
        "| 1     | 0.5575    | 0.3020  | 0.3918   |  851.000   |\n",
        "| 2     | 0.8378    | 0.7762  | 0.8059   | 4572.000   |\n",
        "| **Overall** | | | | **11880.000** |\n",
        "\n",
        "- **Macro Avg Precision**: 73.29%\n",
        "- **Weighted Avg Precision**: 79.90%\n",
        "- **Macro Avg F1-Score**: 68.12%\n",
        "- **Weighted Avg F1-Score**: 79.80%\n",
        "\n",
        "**Analysis:**\n",
        "- The model shows a high accuracy of 80.61%, indicating strong overall performance.\n",
        "- Precision, recall, and F1-score vary significantly across different classes. Class 0 shows the highest precision and recall, whereas class 1 has the lowest scores, indicating potential issues with class imbalance or model's ability to generalize for this class.\n",
        "- The difference in performance across classes suggests a need to further investigate class-specific features or consider techniques to handle imbalanced classes.\n",
        "- The substantial difference between macro and weighted averages points to the influence of class imbalance on the model's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMV7aju56cpg"
      },
      "source": [
        "#Feature Importance (not required)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "_W1hqUXumZkx",
        "outputId": "086cb29b-e218-4ce8-b0af-0146b1a8a349"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get feature importance\n",
        "feature_importances = xgb_tweaked.feature_importances_\n",
        "\n",
        "# Get column names from the training values dataset\n",
        "feature_names = training_values.columns\n",
        "\n",
        "# Create a DataFrame for easy visualization\n",
        "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
        "\n",
        "# Sort the DataFrame by importance\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importance in Tweaked XGBoost Model')\n",
        "plt.gca().invert_yaxis()  # To display the highest importance at the top\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNzx5RqN6XfC"
      },
      "source": [
        "#Creating submission file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqlBq-ZcptpT",
        "outputId": "c6a1dfdd-e666-47b4-917d-2cbc2c386cd7"
      },
      "outputs": [],
      "source": [
        "# Making predictions on the test set\n",
        "test_predictions = xgb_tweaked.predict(X_test_processed)\n",
        "\n",
        "# label names in the original dataset are 'functional', 'non functional', and 'functional needs repair'\n",
        "label_names = ['functional', 'non functional', 'functional needs repair']\n",
        "\n",
        "# Transform the class indices (0, 1, 2) back to original class names\n",
        "test_predictions_labels = [label_names[index] for index in test_predictions]\n",
        "\n",
        "# Creating a DataFrame for submission\n",
        "submission_df = pd.DataFrame({\n",
        "    'id': test_values['id'],  # Assuming 'id' column is present in test_values\n",
        "    'status_group': test_predictions_labels\n",
        "})\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(submission_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DB0LKFQKwWy6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Saving the DataFrame to a CSV file\n",
        "submission_df.to_csv('/content/drive/MyDrive/MMAI_Group/894_team/DrivenData_Competition/notebooks/John/SubmissionFormat.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM3jGcIa1Z9zv80bQJUY3Z9",
      "include_colab_link": true,
      "mount_file_id": "1vNMHTa2cx7PcEIM3LxgHYpWHLKEyickY",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
