{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hellojohnkim/mmai894/blob/main/Pump_it_up_XGB_Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials, atpe\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "tlfiocXYVyce"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pump It Up Faulty Water Pump Prediction Model"
      ],
      "metadata": {
        "id": "CaloidJpXoIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loading"
      ],
      "metadata": {
        "id": "P1rDzc_2YTAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#run this cell if you get denied running the data loading cell\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGYcCSo4uQIa",
        "outputId": "d9260709-3573-44b3-a8ff-61bba60ecc07"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specifying the datasets file paths\n",
        "training_set_values_file_path = '/content/drive/MyDrive/MMAI_Group/894_team/DrivenData_Competition/data/training_set_values.csv'\n",
        "training_set_labels_file_path = '/content/drive/MyDrive/MMAI_Group/894_team/DrivenData_Competition/data/training_set_label.csv'\n",
        "test_set_file_path = '/content/drive/MyDrive/MMAI_Group/894_team/DrivenData_Competition/data/test_set.csv'\n",
        "\n",
        "# Loading the datasets\n",
        "features_df = pd.read_csv(training_set_values_file_path)\n",
        "labels_df = pd.read_csv(training_set_labels_file_path)\n",
        "test = pd.read_csv(test_set_file_path)\n",
        "\n",
        "# Displaying the first few rows of the datasets\n",
        "features_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcRkDDUjGcN6",
        "outputId": "0ffa8fa1-ceae-4335-b68a-8e4843386eab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 59400 entries, 0 to 59399\n",
            "Data columns (total 40 columns):\n",
            " #   Column                 Non-Null Count  Dtype  \n",
            "---  ------                 --------------  -----  \n",
            " 0   id                     59400 non-null  int64  \n",
            " 1   amount_tsh             59400 non-null  float64\n",
            " 2   date_recorded          59400 non-null  object \n",
            " 3   funder                 55765 non-null  object \n",
            " 4   gps_height             59400 non-null  int64  \n",
            " 5   installer              55745 non-null  object \n",
            " 6   longitude              59400 non-null  float64\n",
            " 7   latitude               59400 non-null  float64\n",
            " 8   wpt_name               59400 non-null  object \n",
            " 9   num_private            59400 non-null  int64  \n",
            " 10  basin                  59400 non-null  object \n",
            " 11  subvillage             59029 non-null  object \n",
            " 12  region                 59400 non-null  object \n",
            " 13  region_code            59400 non-null  int64  \n",
            " 14  district_code          59400 non-null  int64  \n",
            " 15  lga                    59400 non-null  object \n",
            " 16  ward                   59400 non-null  object \n",
            " 17  population             59400 non-null  int64  \n",
            " 18  public_meeting         56066 non-null  object \n",
            " 19  recorded_by            59400 non-null  object \n",
            " 20  scheme_management      55523 non-null  object \n",
            " 21  scheme_name            31234 non-null  object \n",
            " 22  permit                 56344 non-null  object \n",
            " 23  construction_year      59400 non-null  int64  \n",
            " 24  extraction_type        59400 non-null  object \n",
            " 25  extraction_type_group  59400 non-null  object \n",
            " 26  extraction_type_class  59400 non-null  object \n",
            " 27  management             59400 non-null  object \n",
            " 28  management_group       59400 non-null  object \n",
            " 29  payment                59400 non-null  object \n",
            " 30  payment_type           59400 non-null  object \n",
            " 31  water_quality          59400 non-null  object \n",
            " 32  quality_group          59400 non-null  object \n",
            " 33  quantity               59400 non-null  object \n",
            " 34  quantity_group         59400 non-null  object \n",
            " 35  source                 59400 non-null  object \n",
            " 36  source_type            59400 non-null  object \n",
            " 37  source_class           59400 non-null  object \n",
            " 38  waterpoint_type        59400 non-null  object \n",
            " 39  waterpoint_type_group  59400 non-null  object \n",
            "dtypes: float64(3), int64(7), object(30)\n",
            "memory usage: 18.1+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Pre-Processing"
      ],
      "metadata": {
        "id": "bkR9TqckYega"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "# Merge the data on id\n",
        "data_df = features_df.merge(labels_df, on='id')\n",
        "\n",
        "# Date feature transformation\n",
        "data_df['date_recorded'] = pd.to_datetime(data_df['date_recorded'])\n",
        "data_df['year_recorded'] = data_df['date_recorded'].dt.year\n",
        "data_df['month_recorded'] = data_df['date_recorded'].dt.month\n",
        "data_df['day_recorded'] = data_df['date_recorded'].dt.day\n",
        "data_df['days_since_recorded'] = (data_df['date_recorded'] - data_df['date_recorded'].min()).dt.days\n",
        "data_df.drop('date_recorded', axis=1, inplace=True)\n",
        "\n",
        "# Encode 'construction_year' as a cyclical feature\n",
        "max_year = data_df['construction_year'].max()\n",
        "data_df['construction_year_sin'] = np.sin(2 * np.pi * data_df['construction_year'] / max_year)\n",
        "data_df['construction_year_cos'] = np.cos(2 * np.pi * data_df['construction_year'] / max_year)\n",
        "\n",
        "# Calculate 'age' as 'year_recorded' - 'construction_year'\n",
        "data_df['age'] = data_df['year_recorded'] - data_df['construction_year']\n",
        "data_df['age'].replace({0: np.nan}, inplace=True)  # Replace zero ages (indicating missing data) with NaN\n",
        "data_df.drop(['construction_year'], axis=1, inplace=True)  # Drop original 'construction_year'\n",
        "\n",
        "# Handling categorical variables\n",
        "categorical_cols = data_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "categorical_cols.remove('status_group')\n",
        "categorical_cols.extend(['region_code', 'district_code'])  # Include region and district codes as categorical\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "    ])\n",
        "\n",
        "X = data_df.drop('status_group', axis=1)\n",
        "y = data_df['status_group']\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('xgb', XGBClassifier(eval_metric='mlogloss', n_jobs=-1))\n",
        "])\n",
        "\n",
        "X.drop('id', axis=1, inplace=True)  # Drop 'id' column\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)"
      ],
      "metadata": {
        "id": "ofFaIrYkIy1x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing Summary\n",
        "\n",
        "#### 1. Data Merging\n",
        "- **Variables Involved**: `features_df` and `labels_df`\n",
        "- **Purpose**: To consolidate feature and label data into a single DataFrame.\n",
        "- **Method Applied**: Merged the two DataFrames on the 'id' column.\n",
        "\n",
        "#### 2. Date Feature Transformation\n",
        "- **Variable Affected**: `date_recorded`\n",
        "- **Purpose**: To extract more granular time-related features.\n",
        "- **Method Applied**: Converted `date_recorded` to a datetime object, then created separate columns for year, month, and day. Also calculated the number of days since the earliest recorded date, then dropped the original `date_recorded` column.\n",
        "\n",
        "#### 3. Cyclical Encoding of 'construction_year'\n",
        "- **Variable Affected**: `construction_year`\n",
        "- **Purpose**: To transform a linear numeric feature into a cyclical one, acknowledging its periodic nature.\n",
        "- **Method Applied**: Calculated sine and cosine transformations of `construction_year` and dropped the original column.\n",
        "\n",
        "#### 4. Age Calculation\n",
        "- **Variable Affected**: `age`\n",
        "- **Purpose**: To derive a meaningful feature representing the age of the infrastructure.\n",
        "- **Method Applied**: Calculated 'age' as the difference between the recorded year and the construction year. Replaced zero values (indicating missing data) with NaN.\n",
        "\n",
        "#### 5. Handling Categorical Variables\n",
        "- **Variables Affected**: Various categorical columns\n",
        "- **Purpose**: To transform categorical data into a format suitable for machine learning algorithms.\n",
        "- **Method Applied**: Identified all categorical columns and extended the list with region and district codes. Applied OneHotEncoder to these categorical variables in a preprocessing pipeline.\n",
        "\n",
        "#### 6. Feature and Label Separation\n",
        "- **Variables Affected**: `X` (features) and `y` (labels)\n",
        "- **Purpose**: To prepare the dataset for model training.\n",
        "- **Method Applied**: Separated the dataset into features (`X`) and labels (`y`), dropping the 'id' column from features and the 'status_group' column from labels.\n",
        "\n",
        "#### 7. Label Encoding\n",
        "- **Variable Affected**: `status_group`\n",
        "- **Purpose**: To convert label data into a numerical format.\n",
        "- **Method Applied**: Applied Label Encoding to the 'status_group' column.\n",
        "\n",
        "This summary encapsulates the key steps undertaken in the data preprocessing phase of your project. It provides a clear and concise overview of each step, along with its purpose and the specific methods used."
      ],
      "metadata": {
        "id": "N9FLsMUsPm98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#XGB Baseline Model"
      ],
      "metadata": {
        "id": "6fv8P8FkZLfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#XGB Baseline + Hyperparameter Tuning using Hyperopt"
      ],
      "metadata": {
        "id": "iis4BsAfZQTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "space = {\n",
        "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
        "    'max_depth': hp.choice('max_depth', range(3, 15)),\n",
        "    'n_estimators': hp.choice('n_estimators', range(50, 300)),\n",
        "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
        "    'gamma': hp.uniform('gamma', 0, 5),\n",
        "    'min_child_weight': hp.uniform('min_child_weight', 0, 10),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 1.0, 4.0),\n",
        "}\n",
        "\n",
        "def objective(params):\n",
        "    pipeline.set_params(xgb__learning_rate=params['learning_rate'],\n",
        "                        xgb__max_depth=params['max_depth'],\n",
        "                        xgb__n_estimators=params['n_estimators'],\n",
        "                        xgb__subsample=params['subsample'],\n",
        "                        xgb__colsample_bytree=params['colsample_bytree'],\n",
        "                        xgb__gamma=params['gamma'],\n",
        "                        xgb__min_child_weight=params['min_child_weight'],\n",
        "                        xgb__reg_alpha=params['reg_alpha'],\n",
        "                        xgb__reg_lambda=params['reg_lambda'])\n",
        "    score = cross_val_score(pipeline, X, y, cv=6, scoring='f1_weighted', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}\n",
        "\n",
        "trials = Trials()\n",
        "best_params = fmin(fn=objective, space=space, algo=atpe.suggest, max_evals=60, trials=trials)\n",
        "\n",
        "print(\"Best parameters:\", best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjM6F6zOv8Z5",
        "outputId": "1dd8337c-bc6c-48f7-e456-6c743636f7ce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 60/60 [19:57:45<00:00, 1197.75s/trial, best loss: -0.7982059849267075]\n",
            "Best parameters: {'colsample_bytree': 0.9993491087620797, 'gamma': 0.37885665466692675, 'learning_rate': 0.17813266334475886, 'max_depth': 11, 'min_child_weight': 0.9243224187937522, 'n_estimators': 219, 'reg_alpha': 0.06930257498733917, 'reg_lambda': 1.502077184383809, 'subsample': 0.6934044303500844}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "100%|██████████| 60/60 [19:57:45<00:00, 1197.75s/trial, best loss: -0.7982059849267075]\n",
        "Best parameters: {'colsample_bytree': 0.9993491087620797, 'gamma': 0.37885665466692675, 'learning_rate': 0.17813266334475886, 'max_depth': 11, 'min_child_weight': 0.9243224187937522, 'n_estimators': 219, 'reg_alpha': 0.06930257498733917, 'reg_lambda': 1.502077184383809, 'subsample': 0.6934044303500844}"
      ],
      "metadata": {
        "id": "bKwVhmd_9xBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#best params from Peter's training:\n",
        "best_params = {'colsample_bytree': 0.9993491087620797, 'gamma': 0.37885665466692675, 'learning_rate': 0.17813266334475886, 'max_depth': 11, 'min_child_weight': 0.9243224187937522, 'n_estimators': 219, 'reg_alpha': 0.06930257498733917, 'reg_lambda': 1.502077184383809, 'subsample': 0.6934044303500844}\n",
        "\n",
        "final_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('xgb', XGBClassifier(eval_metric='mlogloss',\n",
        "                          learning_rate=best_params['learning_rate'],\n",
        "                          max_depth=best_params['max_depth'],\n",
        "                          n_estimators=best_params['n_estimators'],\n",
        "                          subsample=best_params['subsample'],\n",
        "                          colsample_bytree=best_params['colsample_bytree'],\n",
        "                          gamma=best_params['gamma'],\n",
        "                          min_child_weight=best_params['min_child_weight'],\n",
        "                          reg_alpha=best_params['reg_alpha'],\n",
        "                          reg_lambda=best_params['reg_lambda']))\n",
        "])\n",
        "\n",
        "model = final_pipeline.fit(X, y)"
      ],
      "metadata": {
        "id": "Fec-qOccjd_F"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['date_recorded'] = pd.to_datetime(test['date_recorded'])\n",
        "test['year_recorded'] = test['date_recorded'].dt.year\n",
        "test['month_recorded'] = test['date_recorded'].dt.month\n",
        "test['day_recorded'] = test['date_recorded'].dt.day\n",
        "test['days_since_recorded'] = (test['date_recorded'] - test['date_recorded'].min()).dt.days\n",
        "test.drop('date_recorded', axis=1, inplace=True)\n",
        "\n",
        "def encode_cyclical_features(df, cols):\n",
        "    for col in cols:\n",
        "        max_val = df[col].max()\n",
        "        df[col + '_sin'] = np.sin(2 * np.pi * df[col] / max_val)\n",
        "        df[col + '_cos'] = np.cos(2 * np.pi * df[col] / max_val)\n",
        "    return df\n",
        "\n",
        "cyclical_cols = ['year_recorded', 'month_recorded', 'day_recorded']\n",
        "\n",
        "test = encode_cyclical_features(test, cyclical_cols)\n",
        "test.drop(['month_recorded', 'day_recorded'], axis=1, inplace=True)\n",
        "categorical_cols = test.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "predictions = model.predict(test)"
      ],
      "metadata": {
        "id": "jm3zMkfcjik8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = label_encoder.inverse_transform(predictions)"
      ],
      "metadata": {
        "id": "B62GuOAunCM5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final = pd.DataFrame({'Id': test.id, 'status_group': predictions})"
      ],
      "metadata": {
        "id": "O1bPDUv-nEC6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving to a CSV file\n",
        "final.to_csv('/content/drive/MyDrive/MMAI_Group/894_team/DrivenData_Competition/notebooks/John/SubmissionFormat_1.csv', index=False)"
      ],
      "metadata": {
        "id": "XArD0njTnFqJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final.rename(columns={'Id': 'id'}, inplace=True)\n",
        "final.rename(columns={'status_group': 'status_group'}, inplace=True)"
      ],
      "metadata": {
        "id": "X_c5t9KfnHMU"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}